\documentclass{article}

\usepackage{corl_2020} % Use this for the initial submission.
%\usepackage[final]{corl_2020} % Uncomment for the camera-ready ``final'' version.
%\usepackage[preprint]{corl_2020} % Uncomment for pre-prints (e.g., arxiv); This is like ``final'', but will remove the CORL footnote.

\title{Formatting Instructions for CoRL 2020}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

% NOTE: authors will be visible only in the camera-ready and preprint versions (i.e., when using the option 'final' or 'preprint'). 
% 	For the initial submission the authors will be anonymized.

\author{
  Jane E.~Doe\\
  Department of Electrical Engineering and Computer Sciences\\
  University of California Berkeley 
  United States\\
  \texttt{janedoe@berkeley.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle

%===============================================================================

\begin{abstract}
    Recent success in offline Reinforcement Learning (RL) is highlighted by its adaptability to novel scenarios. One of the key reasons behind this success is the readily available nature of behavior transitions. Practical applications, on the other hand, consist of behavior policy as a sequence of demonstrations rather than a dataset of transitions. This allows one to rethink \textit{behavior initialization} through the lens of imitation. We steer research towards this direction by answering the central question of \textit{how can demonstrations be utilized for behavior initialization?} Our study aims to combine Imitation Learning (IL) as the behavioral aspect for offline RL. We aim to explore theoretical properties of IL-RL combinations and empirically evaluate them in light of data efficiency on a suite of locomotion and manipulation tasks. The study additionally aims to throw light on the various tradeoffs between data collection and optimal behavior and why striking a balance between the two is essential from a practical standpoint. We hope that our work serves as a motivating example for application of offline RL to practical problems.
\end{abstract}

% Two or three meaningful keywords should be added here
\keywords{CoRL, Robots, Learning} 

%===============================================================================

\section{Problem}

Consider the scenario wherein a human learns to drive a car. The driver observes a teacher driving the car. This involves paying attention to crucial insights of controlling the vehicle such as steering while making a turn, accelerating during a green light and monitoring mirrors during brakes. Increasing amount of observations by the driver result in learning finer details of the task which are not available \textit{apriori}. For instance, the driver may never learn to make a U-turn if the teacher never encountered a U-turn crossing. 

Offline RL addresses this intuitive gap in learning by equiping an agent (the \textit{driver} in above example) with the ability to stitch together portions of observations by making use of a dataset of transitions. For instance, the driver may learn to make a U-turn on its own if it observes the teacher making sharp turns and slowing down the vehicle at intersections. Adoption of transitions in the offline setting allows the agent to efficaciously tackle distributional shift between the teacher's policy and the agent's policy. To this end, it is reasonable to ask the question \textit{how would the agent learn to make a U-turn if it never saw the teacher make sharp turns or slow down the vehicle?} More specifically, how does the agent stitch together portions of transitions with limited data?

Various scenarios make data collection imperative in the face of uncertainty. Lack of optimal behavior transitions observed by an offline RL agent may cripple its policy and result in suboptimal convergence. This allows one to rethink offline RL as an abstraction of \textit{behavior initialization} and \textit{learning} problems. In the first stage, the agent desires a suitable initialization point (a behavior policy or static dataset of transitions) which would serve as a guididng principle for agent's policy. The second stage comprises of learning optimal behaviors based on initialization point. The direct dependence of offline mechanims on behavior initialization highlights its pivotal role in the learning pipeline.

Modern offline RL methods resort to a black-box dataset of transitions as the initialization point. This often limits optimal behavior at the cost of data collection (as observed in \textit{driving} example). A suitable alternative to address this limitation is by utilizing demonstrations as initialization point for the agent's policy. Similar to static transitions in offline RL, expert demonstrations provide a guiding mechanism for learner's policy in the IL setup. A suitable combination of IL-RL which trades off data collection with optimal behavior forms the center of our study. 

% no \bibliographystyle is required, since the corl style is automatically used.
\bibliography{example}  % .bib

\end{document}
